# AI Research Notes

Tracking interesting papers and concepts in AI/ML.

## Recent Papers

**Attention Is All You Need (Transformer Architecture)**
- Revolutionary paper introducing the transformer architecture [1]
- Self-attention mechanism replaces recurrence
- Foundation for GPT, BERT, and modern LLMs

**LoRA: Low-Rank Adaptation**
- Efficient fine-tuning method for large language models [2]
- Significantly reduces trainable parameters
- Maintains performance while cutting compute costs

**Retrieval-Augmented Generation (RAG)**
- Combines retrieval with generation for better factual grounding
- Reduces hallucinations in LLM outputs
- Key architecture for LocalBrain-style applications

## Current Interests

- Local-first AI systems
- Efficient model fine-tuning
- Privacy-preserving ML
- Vector databases and semantic search
- Model Context Protocol (MCP)

## UCR AI Lab Work

Working on efficient fine-tuning methods for domain adaptation [3]. Focus on reducing computational requirements while maintaining model quality.


## Work in Progress

## Work in Progress

**Vector Search Implementation for LocalBrain**
- Started implementation work on vector search functionality
- Set up ChromaDB for vector storage and retrieval
- Implemented basic embedding generation using sentence-transformers library


## Interview Updates

## Interview Updates

Received final interview date from Amazon for Senior SWE role on November 18th [4].

## Related

- [[LocalBrain]]
- [[Learning Goals]]
